{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pipeline as pl\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structure Modification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given that the dataset has continuous relevance score, neural network model have treated as a regression problem. However, due to the high rmse results, this is an attempt to restructure the dataset into a classification problem and see if neural network model works better. \n",
    "* The dataset has been restructured into the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label | Original Value\n",
    "------------- |:-------------\n",
    "0 | 1\n",
    "1 | 1.25, 1.3300000000000001\n",
    "2 | 1.5, 1.6699999999999999\n",
    "3 | 1.75, 2.0\n",
    "4 | 2.25, 2.3300000000000001\n",
    "5 | 2.5, 2.6699999999999999\n",
    "6 | 2.75, 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def balanced(train):\n",
    "    downsize = train[train['relevance'] == 2.3300000000000001].sample(n = 1760, random_state = 2017)\n",
    "    downsize = pd.concat([downsize, train[train['relevance'] == 2.6699999999999999].sample(n = 1760,\\\n",
    "                                                                                           random_state = 2017),\\\n",
    "                          train[train['relevance'] == 3.0].sample(n = 1760,random_state = 2017),\\\n",
    "                          train[train['relevance'] == 2.0].sample(n = 1760,random_state = 2017),\\\n",
    "                         train[train['relevance'] == 1.6699999999999999].sample(n = 1760,random_state = 2017),\n",
    "                         train[train['relevance'] == 1.3300000000000001].sample(n = 1760,random_state = 2017)]\\\n",
    "                         ,axis = 0)\n",
    "    rest_ = train.loc[train['relevance'] != 2.3300000000000001]\n",
    "    rest_1 = rest_.loc[rest_['relevance'] != 1.3300000000000001]\n",
    "    rest_1_ = rest_1.loc[rest_1['relevance'] != 1.6699999999999999]\n",
    "    rest_2 = rest_1_.loc[rest_1_['relevance'] != 2.6699999999999999]\n",
    "    rest_3 = rest_2.loc[rest_2['relevance'] != 3.0]\n",
    "    rest_data = rest_3.loc[rest_3['relevance'] != 2.0]\n",
    "    down_train = pd.concat([rest_data, downsize], 0)\n",
    "    return down_train\n",
    "\n",
    "def classified(data):\n",
    "    zero = data[data['relevance'] == 1].copy()\n",
    "    zero['relevance'] = 0\n",
    "    one = data[(data['relevance'] == 1.25) | (data['relevance'] == 1.3300000000000001)].copy()\n",
    "    one['relevance'] = 1\n",
    "    two = data[(data['relevance'] == 1.5) | (data['relevance'] == 1.6699999999999999)].copy()\n",
    "    two['relevance'] = 2\n",
    "    three = data[(data['relevance'] == 1.75) | (data['relevance'] == 2.0)].copy()\n",
    "    three['relevance'] = 3\n",
    "    four = data[(data['relevance'] == 2.25) | (data['relevance'] == 2.3300000000000001)].copy()\n",
    "    four['relevance'] = 4\n",
    "    five = data[(data['relevance'] == 2.5) | (data['relevance'] == 2.6699999999999999)].copy()\n",
    "    five['relevance'] = 5\n",
    "    six = data[(data['relevance'] == 2.75) | (data['relevance'] == 3.0)].copy()\n",
    "    six['relevance'] = 6\n",
    "    classified = pd.concat([zero,one,two,three,four,five,six], 0)\n",
    "    return classified"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train = pd.read_csv('temp/withoneattr_train.csv')\n",
    "dev = pd.read_csv('temp/withoneattr_dev.csv')\n",
    "b_data = balanced(train)\n",
    "c_data = classified(b_data)\n",
    "c_dev = classified(dev)\n",
    "c_data.to_csv('temp/classification/train.csv',index=False)\n",
    "c_dev.to_csv('temp/classification/dev.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corpus(filename='temp/classification/train.csv'):\n",
    "    import csv\n",
    "    reader = csv.reader(open(filename))\n",
    "    data = []\n",
    "    for row in reader:\n",
    "        i_d,product_uid,product_title,search_term,relevance,name,value = row\n",
    "        dic = {'search_term':search_term, 'product_title': product_title,'relevance': relevance, 'attr_1':value}\n",
    "        data.append(dic)\n",
    "    del data[0]\n",
    "    return data\n",
    "\n",
    "def tokenise(input):\n",
    "    return input.split(' ')\n",
    "\n",
    "def pipeline(data,vocab=None, max_title_len_=None, max_query_len_=None,max_attr_len_=None):\n",
    "    \n",
    "    # Vocab\n",
    "    exist_vocab = True\n",
    "    if vocab is None:\n",
    "        exist_vocab = False\n",
    "        vocab = {'<PAD>': 0, '<OOV>':1}\n",
    "    \n",
    "    ## Placeholder ## \n",
    "    \n",
    "    # Product Title\n",
    "    max_title_len = -1\n",
    "    data_title = []\n",
    "    # Search Term\n",
    "    max_query_len = -1\n",
    "    data_query = []\n",
    "    # Attr Term\n",
    "    max_attr_len = -1\n",
    "    data_attr = []\n",
    "    \n",
    "    # Relevance\n",
    "    data_relevance = []\n",
    "    \n",
    "    ## Processing ## \n",
    "    for instance in data:\n",
    "        # Product Title Processing\n",
    "        title = instance['product_title']\n",
    "        t_tokenised = tokenise(title)\n",
    "        each_title = []\n",
    "        for token in t_tokenised:\n",
    "            if not exist_vocab and token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "            if token not in vocab:\n",
    "                token_id = vocab['<OOV>']\n",
    "            else:\n",
    "                token_id = vocab[token]\n",
    "            each_title.append(token_id)\n",
    "        # Product Title Length\n",
    "        if len(each_title) > max_title_len:\n",
    "            max_title_len = len(each_title)\n",
    "        data_title.append(each_title)\n",
    "            \n",
    "        # Search Term Processing\n",
    "        query = instance['search_term']\n",
    "        q_tokenised = tokenise(query)\n",
    "        each_query = []\n",
    "        for qtoken in q_tokenised:\n",
    "            if not exist_vocab and qtoken not in vocab:\n",
    "                vocab[qtoken] = len(vocab)\n",
    "            if qtoken not in vocab:\n",
    "                qtoken_id = vocab['<OOV>']\n",
    "            else:\n",
    "                qtoken_id = vocab[qtoken]\n",
    "            each_query.append(qtoken_id) \n",
    "        # Search Terms Length\n",
    "        if len(each_query) > max_query_len:\n",
    "            max_query_len = len(each_query)\n",
    "        data_query.append(each_query)\n",
    "        \n",
    "\n",
    "        # Attr Processing\n",
    "        attr = instance['attr_1']\n",
    "        a_tokenised = tokenise(attr)\n",
    "        each_attr = []\n",
    "        for atoken in a_tokenised:\n",
    "            if not exist_vocab and atoken not in vocab:\n",
    "                vocab[atoken] = len(vocab)\n",
    "            if atoken not in vocab:\n",
    "                atoken_id = vocab['<OOV>']\n",
    "            else:\n",
    "                atoken_id = vocab[atoken]\n",
    "            each_attr.append(atoken_id) \n",
    "        # Search Terms Length\n",
    "        if len(each_attr) > max_attr_len:\n",
    "            max_attr_len = len(each_attr)\n",
    "        data_attr.append(each_attr)\n",
    "            \n",
    "        # Relevance \n",
    "        data_relevance.append(instance['relevance'])\n",
    "\n",
    "    if max_title_len_ is not None:\n",
    "        max_title_len = max_title_len_\n",
    "    out_title = np.full([len(data_title), max_title_len], vocab['<PAD>'], dtype=np.int32)\n",
    "    \n",
    "    for index, item in enumerate(data_title):\n",
    "        if len(item) <= out_title.shape[1]:\n",
    "            out_title[index, 0:len(item)] = item\n",
    "    \n",
    "    if max_query_len_ is not None:\n",
    "        max_query_len = max_query_len_\n",
    "    out_query = np.full([len(data_query), max_query_len], vocab['<PAD>'], dtype=np.int32)\n",
    "    \n",
    "    for index, q in enumerate(data_query):\n",
    "        out_query[index, 0:len(q)] = q\n",
    "        \n",
    "    if max_attr_len_ is not None:\n",
    "        max_attr_len = max_attr_len_\n",
    "    out_attr = np.full([len(data_attr), max_attr_len], vocab['<PAD>'], dtype=np.int32)\n",
    "    \n",
    "    for index, item in enumerate(data_attr):\n",
    "        if len(item) <= out_title.shape[1]:\n",
    "            out_attr[index, 0:len(item)] = item\n",
    "        \n",
    "    out_relevance = np.array(data_relevance, dtype=np.float64)\n",
    "    \n",
    "    return out_title, out_query, out_relevance, out_attr , vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the corpus\n",
    "\n",
    "train = corpus()\n",
    "test = corpus(filename='temp/classification/dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert train set to integer IDs\n",
    "train_title, train_query, train_relevance,train_attr, vocab = pipeline(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the length of the longest title\n",
    "max_title_len = train_title.shape[1]\n",
    "\n",
    "# get the length of the longest query\n",
    "max_query_len = train_query.shape[1]\n",
    "\n",
    "# get the length of the longest query\n",
    "max_attr_len = train_attr.shape[1]\n",
    "\n",
    "# convert dev set to integer IDs, based on the train's information\n",
    "dev_title, dev_query, dev_relevance,dev_attr,_ = pipeline(test,vocab=vocab,max_query_len_= max_query_len, \\\n",
    "                                                                max_title_len_ = max_title_len\\\n",
    "                                                               ,max_attr_len_ = max_attr_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### MODEL PARAMETERS ###\n",
    "target_size = 1\n",
    "vocab_size = len(vocab)\n",
    "#qvocab_size = len(qvocab)\n",
    "#avocab_size = len(avocab)\n",
    "input_size = 10\n",
    "hidden_size = 30\n",
    "output_size = 1\n",
    "num_of_layers = 3\n",
    "prob_keep = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PLACEHOLDERS\n",
    "\n",
    "title = tf.placeholder(tf.int64, [None, None], \"title\")        # [batch_size x max_title_length]\n",
    "query = tf.placeholder(tf.int64, [None, None], \"query\")        # [batch_size x max_query_length]\n",
    "attr = tf.placeholder(tf.int64, [None, None], \"attr\")       # [batch_size x max_attr_length]\n",
    "relevance = tf.placeholder(tf.int64, [None], \"relevance\")      # [batch_size]\n",
    "\n",
    "#batch_size = tf.shape(title)[0]\n",
    "\n",
    "# WORD EMBEDDINGS\n",
    "\n",
    "initializer = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "embeddings = tf.get_variable(\"E\", [vocab_size, input_size], initializer=initializer)\n",
    "title_embedded = tf.nn.embedding_lookup(embeddings, title)     # [batch_size x max_title_length x input_size]\n",
    "query_embedded = tf.nn.embedding_lookup(embeddings, query)     # [batch_size x max_query_length x input_size]\n",
    "attr_embedded = tf.nn.embedding_lookup(embeddings, attr)       # [batch_size x max_attr_length x input_size]\n",
    "\n",
    "# MODEL CONSTRUCTION\n",
    "\n",
    "with tf.variable_scope(\"query\") as varscope:\n",
    "    cell_ = tf.contrib.rnn.LSTMCell(hidden_size,state_is_tuple=True)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell=cell_, output_keep_prob=prob_keep)\n",
    "    mcell = tf.contrib.rnn.MultiRNNCell([cell]*num_of_layers, state_is_tuple=True)\n",
    "    output, final_state_q = tf.nn.dynamic_rnn(mcell, query_embedded, dtype=tf.float32)\n",
    "\n",
    "with tf.variable_scope(\"title\") as varscope:\n",
    "    cell_t = tf.contrib.rnn.LSTMCell(hidden_size,state_is_tuple=True)\n",
    "    cellt = tf.contrib.rnn.DropoutWrapper(cell=cell_t, output_keep_prob=prob_keep)\n",
    "    mcellt = tf.contrib.rnn.MultiRNNCell([cellt]*num_of_layers, state_is_tuple=True)\n",
    "    output, final_state_t = tf.nn.dynamic_rnn(mcellt, query_embedded,initial_state = final_state_q, dtype=tf.float32)\n",
    "\n",
    "with tf.variable_scope(\"attr\") as varscope:\n",
    "    cell_a = tf.contrib.rnn.LSTMCell(hidden_size,state_is_tuple=True)\n",
    "    cella = tf.contrib.rnn.DropoutWrapper(cell=cell_a, output_keep_prob=prob_keep)\n",
    "    mcella = tf.contrib.rnn.MultiRNNCell([cella]*num_of_layers, state_is_tuple=True)\n",
    "    output, final_state_a = tf.nn.dynamic_rnn(mcella, query_embedded,initial_state = final_state_t, dtype=tf.float32)\n",
    "    joint_h = final_state_a[num_of_layers - 1].h\n",
    "    \n",
    "# LOSS FUNCTION \n",
    "logits_flat = tf.contrib.layers.linear(joint_h, 7) # 7 is the number of classes \n",
    "logits = tf.reshape(logits_flat, [-1, 7])          # [BATCH_SIZE x 7]\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = relevance))\n",
    "\n",
    "# Prediction function\n",
    "softmax = tf.nn.softmax(logits)\n",
    "predict = tf.arg_max(softmax,1)\n",
    "\n",
    "# OPTIMIZER\n",
    "opt_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def save_model(session):\n",
    "    if not os.path.exists('./model/'):\n",
    "        os.mkdir('./model/')\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(session, './model/model.checkpoint')\n",
    "'''\n",
    "# Gradient Clipping \n",
    "optimizer = tf.train.FtrlOptimizer(0.5)\n",
    "gvs = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "opt_op = optimizer.apply_gradients(capped_gvs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(true_relevance,predicted_relevance):\n",
    "    num_correct = np.sum(true_relevance == predicted_relevance)\n",
    "    num_total =  true_relevance.shape[0]\n",
    "    return num_correct / num_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Epoch 0 -----\n",
      " Train loss:  0.0174351349863\n",
      " Train Accuracy:  0.143099547511\n",
      " Dev Accuracy:  0.218943661972\n",
      "----- Epoch 1 -----\n",
      " Train loss:  0.0151850038702\n",
      " Train Accuracy:  0.143099547511\n",
      " Dev Accuracy:  0.218943661972\n",
      "----- Epoch 2 -----\n",
      " Train loss:  0.0132748848616\n",
      " Train Accuracy:  0.143261150614\n",
      " Dev Accuracy:  0.219072769953\n",
      "----- Epoch 3 -----\n",
      " Train loss:  0.012887986696\n",
      " Train Accuracy:  0.145685197156\n",
      " Dev Accuracy:  0.223943661972\n",
      "----- Epoch 4 -----\n",
      " Train loss:  0.0126389788094\n",
      " Train Accuracy:  0.143745959922\n",
      " Dev Accuracy:  0.226737089202\n",
      "----- Epoch 5 -----\n",
      " Train loss:  0.0117607764375\n",
      " Train Accuracy:  0.14067550097\n",
      " Dev Accuracy:  0.220950704225\n",
      "----- Epoch 6 -----\n",
      " Train loss:  0.00915638474192\n",
      " Train Accuracy:  0.142533936652\n",
      " Dev Accuracy:  0.215845070423\n",
      "----- Epoch 7 -----\n",
      " Train loss:  0.00839146358125\n",
      " Train Accuracy:  0.145281189399\n",
      " Dev Accuracy:  0.197429577465\n",
      "----- Epoch 8 -----\n",
      " Train loss:  0.0081925446164\n",
      " Train Accuracy:  0.144957983193\n",
      " Dev Accuracy:  0.215598591549\n",
      "----- Epoch 9 -----\n",
      " Train loss:  0.00756525790106\n",
      " Train Accuracy:  0.144392372334\n",
      " Dev Accuracy:  0.214201877934\n",
      "----- Epoch 10 -----\n",
      " Train loss:  0.011275164612\n",
      " Train Accuracy:  0.143180349063\n",
      " Dev Accuracy:  0.220610328638\n",
      "----- Epoch 11 -----\n",
      " Train loss:  0.0142814387009\n",
      " Train Accuracy:  0.138009049774\n",
      " Dev Accuracy:  0.21867370892\n",
      "----- Epoch 12 -----\n",
      " Train loss:  0.0154563679583\n",
      " Train Accuracy:  0.145604395604\n",
      " Dev Accuracy:  0.132746478873\n",
      "----- Epoch 13 -----\n",
      " Train loss:  0.011693637493\n",
      " Train Accuracy:  0.142695539754\n",
      " Dev Accuracy:  0.217687793427\n",
      "----- Epoch 14 -----\n",
      " Train loss:  0.0112133793624\n",
      " Train Accuracy:  0.140998707175\n",
      " Dev Accuracy:  0.219753521127\n",
      "----- Epoch 15 -----\n",
      " Train loss:  0.0127595223982\n",
      " Train Accuracy:  0.139786683904\n",
      " Dev Accuracy:  0.217699530516\n",
      "----- Epoch 16 -----\n",
      " Train loss:  0.0124252341837\n",
      " Train Accuracy:  0.144230769231\n",
      " Dev Accuracy:  0.21515258216\n",
      "----- Epoch 17 -----\n",
      " Train loss:  0.0107786792903\n",
      " Train Accuracy:  0.142372333549\n",
      " Dev Accuracy:  0.215622065728\n",
      "----- Epoch 18 -----\n",
      " Train loss:  0.0226662324353\n",
      " Train Accuracy:  0.14358435682\n",
      " Dev Accuracy:  0.2104342723\n",
      "----- Epoch 19 -----\n",
      " Train loss:  0.021124407389\n",
      " Train Accuracy:  0.144715578539\n",
      " Dev Accuracy:  0.0486150234742\n",
      "----- Epoch 20 -----\n",
      " Train loss:  0.0167007543286\n",
      " Train Accuracy:  0.145038784745\n",
      " Dev Accuracy:  0.131607981221\n",
      "----- Epoch 21 -----\n",
      " Train loss:  0.0148903776945\n",
      " Train Accuracy:  0.140513897867\n",
      " Dev Accuracy:  0.208661971831\n",
      "----- Epoch 22 -----\n",
      " Train loss:  0.0143945227535\n",
      " Train Accuracy:  0.144553975436\n",
      " Dev Accuracy:  0.217852112676\n",
      "----- Epoch 23 -----\n",
      " Train loss:  0.0186974817252\n",
      " Train Accuracy:  0.142937944409\n",
      " Dev Accuracy:  0.217957746479\n",
      "----- Epoch 24 -----\n",
      " Train loss:  0.0152531668602\n",
      " Train Accuracy:  0.145119586296\n",
      " Dev Accuracy:  0.216737089202\n",
      "----- Epoch 25 -----\n",
      " Train loss:  0.0143493829029\n",
      " Train Accuracy:  0.149321266968\n",
      " Dev Accuracy:  0.213274647887\n",
      "----- Epoch 26 -----\n",
      " Train loss:  0.0139164348418\n",
      " Train Accuracy:  0.146654815772\n",
      " Dev Accuracy:  0.218427230047\n",
      "----- Epoch 27 -----\n",
      " Train loss:  0.012525533211\n",
      " Train Accuracy:  0.146654815772\n",
      " Dev Accuracy:  0.222112676056\n",
      "----- Epoch 28 -----\n",
      " Train loss:  0.0120550547597\n",
      " Train Accuracy:  0.143988364577\n",
      " Dev Accuracy:  0.216197183099\n",
      "----- Epoch 29 -----\n",
      " Train loss:  0.0106974768404\n",
      " Train Accuracy:  0.14479638009\n",
      " Dev Accuracy:  0.20970657277\n",
      "----- Epoch 30 -----\n",
      " Train loss:  0.0102049917195\n",
      " Train Accuracy:  0.14358435682\n",
      " Dev Accuracy:  0.20103286385\n",
      "----- Epoch 31 -----\n",
      " Train loss:  0.00980300355146\n",
      " Train Accuracy:  0.145604395604\n",
      " Dev Accuracy:  0.210657276995\n",
      "----- Epoch 32 -----\n",
      " Train loss:  0.0112326860804\n",
      " Train Accuracy:  0.146089204913\n",
      " Dev Accuracy:  0.221772300469\n",
      "----- Epoch 33 -----\n",
      " Train loss:  0.0114151920471\n",
      " Train Accuracy:  0.1424531351\n",
      " Dev Accuracy:  0.218427230047\n",
      "----- Epoch 34 -----\n",
      " Train loss:  0.0126125317455\n",
      " Train Accuracy:  0.143341952165\n",
      " Dev Accuracy:  0.21896713615\n",
      "----- Epoch 35 -----\n",
      " Train loss:  0.0184904007457\n",
      " Train Accuracy:  0.143099547511\n",
      " Dev Accuracy:  0.218943661972\n",
      "----- Epoch 36 -----\n",
      " Train loss:  0.0148898819154\n",
      " Train Accuracy:  0.142614738203\n",
      " Dev Accuracy:  0.218861502347\n",
      "----- Epoch 37 -----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-bd21e2ac74b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0minst_relevance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_relevance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minst_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevance\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minst_relevance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minst_query\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minst_attr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopt_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcurrent_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fransilavong/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fransilavong/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fransilavong/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/fransilavong/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fransilavong/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "EPOCH = 50\n",
    "train_loss_list = []\n",
    "train_rmse_list = []\n",
    "train_pred_list = []\n",
    "dev_rmse_list = []\n",
    "dev_pred_list = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    n = train_title.shape[0]\n",
    "    \n",
    "    # index to draw random batch of train data\n",
    "    idx = np.arange(n)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        print('----- Epoch', epoch, '-----')\n",
    "        total_loss = 0\n",
    "        for i in range(n // BATCH_SIZE):\n",
    "            inst_title = train_title[idx[i * BATCH_SIZE: (i + 1) * BATCH_SIZE]]\n",
    "            inst_query = train_query[idx[i * BATCH_SIZE: (i + 1) * BATCH_SIZE]]\n",
    "            inst_attr = train_attr[idx[i * BATCH_SIZE: (i + 1) * BATCH_SIZE]]\n",
    "            inst_relevance = train_relevance[idx[i * BATCH_SIZE: (i + 1) * BATCH_SIZE]]\n",
    "            feed_dict = {title: inst_title, relevance: inst_relevance, query: inst_query,attr: inst_attr}\n",
    "            _, current_loss = sess.run([opt_op, loss], feed_dict=feed_dict)\n",
    "            total_loss += current_loss\n",
    "        train_loss_list.append(total_loss/n)\n",
    "        print(' Train loss: ', total_loss / n)\n",
    "\n",
    "        train_feed_dict = {title: train_title, relevance: train_relevance, query: train_query,attr: train_attr}\n",
    "        train_predicted = sess.run(predict, feed_dict=train_feed_dict)\n",
    "        train_pred_list.append(train_predicted)\n",
    "        train_accuracy = calculate_accuracy(train_relevance, train_predicted)\n",
    "        train_rmse_list.append(train_accuracy)\n",
    "        print(' Train Accuracy: ', train_accuracy)\n",
    "        \n",
    "        dev_feed_dict = {title: dev_title, relevance: dev_relevance, query:dev_query,attr:dev_query}\n",
    "        dev_predicted = sess.run(predict, feed_dict=dev_feed_dict)\n",
    "        dev_pred_list.append(dev_predicted)\n",
    "        dev_accuracy = calculate_accuracy(dev_relevance, dev_predicted)\n",
    "        dev_rmse_list.append(dev_accuracy)\n",
    "        print(' Dev Accuracy: ', dev_accuracy)\n",
    "\n",
    "    \n",
    "    #save_model(sess)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
